{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a75205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (449 kB)\n",
      "\u001b[K     |████████████████████████████████| 449 kB 77.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml>=4.1.0\n",
      "  Downloading lxml-4.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 80.1 MB/s eta 0:00:01     |█████████                       | 1.9 MB 80.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /home/yeoai/anaconda3/envs/pytorch/lib/python3.8/site-packages (from konlpy) (1.20.3)\n",
      "Installing collected packages: lxml, JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0 lxml-4.8.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2043da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca83c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "드론 활용 범위도 점차 확대되고 있다. 최근에는 미세먼지 관리에 드론이 활용되고 있다.\n",
    "서울시는 '미세먼지 계절관리제' 기간인 지난달부터 오는 3월까지 4개월간 드론에 측정장치를 달아 미세먼지 집중 관리를 실시하고 있다.\n",
    "드론은 산업단지와 사업장 밀집지역을 날아다니며 미세먼지 배출 수치를 점검하고, 현장 모습을 영상으로 담는다.\n",
    "영상을 통해 미세먼지 방지 시설을 제대로 가동하지 않는 업체와 무허가 시설에 대한 단속이 한층 수월해질 전망이다.\n",
    "드론 활용에 가장 적극적인 소방청은 광범위하고 복합적인 재난 대응 차원에서 드론과 관련 전문인력 보강을 꾸준히 이어가고 있다.\n",
    "지난해 말 기준 소방청이 보유한 드론은 총 304대, 드론 조종 자격증을 갖춘 소방대원의 경우 1,860명이다.\n",
    "이 중 실기평가지도 자격증까지 갖춘 ‘드론 전문가’ 21명도 배치돼 있다.\n",
    "소방청 관계자는 \"소방드론은 재난현장에서 영상정보를 수집, 산악ㆍ수난 사고 시 인명수색·구조활동,\n",
    "유독가스·폭발사고 시 대원안전 확보 등에 활용된다\"며\n",
    "\"향후 화재진압, 인명구조 등에도 드론을 활용하기 위해 연구개발(R&D)을 하고 있다\"고 말했다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba97e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력: [('\\n', 'Foreign'), ('드론', 'Noun'), ('활용', 'Noun'), ('범위', 'Noun'), ('도', 'Josa'), ('점차', 'Noun'), ('확대', 'Noun'), ('되고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n",
      "명사 추출 : 드론 활용 범위 점차 확대 최근 미세먼지 관리 드론 활용 서울시 미세먼지 계절 관리제 기간 지난달 개 월간 드론 측정 장치 달 미세먼지 집중 관리 실시 드론 산업 단지 사업 밀집 지역 미세먼지 배출 수치 점검 현장 모습 영상 영상 통해 미세먼지 방지 시설 제대로 가동 업체 무허가 시설 대한 단속 한층 전망 드론 활용 가장 적극 소방청 복합 재난 대응 차원 드론 관련 전문 인력 보강 어가 지난해 말 기준 소방청 보유 드론 총 드론 조종 자격증 소방대 경우 명 이 중 실기 평가 지도 자격증 드론 전문가 명도 배치 소방청 관계자 소방 드론 재난 현장 영상 정보 수집 산악 수난 사고 시 인명 수색 구조 활동 유독가스 폭발사고 시 대원 안전 확보 등 활용 며 향후 화재 진압 인명구조 등 드론 활용 위해 연구개발 고 말\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "tokenized_doc = okt.pos(doc)\n",
    "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "\n",
    "print('품사 태깅 10개만 출력:', tokenized_doc[:10])\n",
    "print('명사 추출 :', tokenized_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5a04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 222\n",
      "trigram 다섯개만 출력 : ['가동 업체' '가동 업체 무허가' '가장 적극' '가장 적극 소방청' '경우 실기']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :', len(candidates))\n",
    "print('trigram 다섯개만 출력 :', candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be406333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f5de7db5b4105b8bf88ed8c0b8006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b336860b1064b2e8ca793a851dbde6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ae58da073b43e389a96fe0f83a8550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2564db08bcb14a4abbf1d1e3aef1c99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c892bf9b6e40909c9e39de526021d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8580a46752564e6da9a6a1f45a961a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf98f7f7831433bad34014997b0aa55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7a1df9d9464c24945da1a9f1c01a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6103d722722242d69e8a079e7a465275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dbe127f6f34d63b9ede8d25b4396eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f28baaa144243bc7aa6b1f61d6547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98505fdf2625428c9731225ce39b6990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e883a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['드론 산업', '드론 드론 조종', '실시 드론 산업', '관리 드론 활용', '미세먼지 관리 드론']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n :]]\n",
    "\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d3a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Sum Similiarity\n",
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates) :\n",
    "    \n",
    "    # 문서와 각 키워드간 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    \n",
    "    # 각 키워드 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings,\n",
    "                                            candidate_embeddings)\n",
    "    \n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 선택\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "    \n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf \n",
    "    candidate = None\n",
    "    \n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n) :\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim :\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "            \n",
    "            \n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5007b08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['드론 산업 단지', '전망 드론 활용', '드론 산업', '관리 드론 활용', '미세먼지 관리 드론']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 10개 키워드를 선택하고 10개 중에서 서로 가장 유사성이 낮은 5개를 선택\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f91f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소방 드론 재난', '자격증 드론 전문가', '월간 드론 측정', '전망 드론 활용', '미세먼지 관리 드론']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beed68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity) :\n",
    "    \n",
    "    # 문서와 각 키워드 간 유사도가 적혀진 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    \n",
    "    # 각 키워드 간 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "    \n",
    "    # 문서와 가장 높은 유사도를 가진 키워드 인덱스 추출\n",
    "    # 만약 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    \n",
    "    # 가장 높은 유사도를 가진 키워드 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 .. 중략 ..]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "    \n",
    "    # 최고 키워드는 이미 추출했으므로 top_n -1번만큼 아래를 반복\n",
    "    # ex ) top_n = 5라면, 아래 loop는 4번 반복됨\n",
    "    \n",
    "    for _ in range(top_n - 1) :\n",
    "        candidates_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis= 1)\n",
    "        \n",
    "        # MMR 계산\n",
    "        mmr = (1-diversity) * candidates_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "        \n",
    "        # keywords & candidates 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "        \n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a2df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미세먼지 관리 드론', '실시 드론 산업', '관리 드론 활용', '월간 드론 측정', '전망 드론 활용']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n = 5, diversity = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54fb3cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미세먼지 관리 드론', '사업 밀집', '재난 현장 영상', '산악 수난', '수치 점검']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diversity 값을 높게 하면 다양한 키워드 5개 생성\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n = 5, diversity = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04672c3",
   "metadata": {},
   "source": [
    "<참고>\n",
    "- https://wikidocs.net/159468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405fcc9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8967aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7236bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SUM_SIM Result: ['가장 마지막 위기', '체계 준비 범위', '진자 수가 최근', '전반 대응 과정', '준비 범위 대응', '마지막 위기 전망', '수가 최근 주일']\n",
      "MMR Result: ['대응 마지막 위기', '정부 복수 연구기관', '신규 진자', '사회 전략', '수렴 정부', '예측 형성 의료', '위원회 의견 수렴']\n"
     ]
    }
   ],
   "source": [
    "# 문서 keyword 추출 함수화\n",
    "\n",
    "# Max Sum Similiarity\n",
    "#max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)\n",
    "# Maximal Marginal Relevance\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity) :\n",
    "    \n",
    "    # 문서와 각 키워드 간 유사도가 적혀진 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    \n",
    "    # 각 키워드 간 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "    \n",
    "    # 문서와 가장 높은 유사도를 가진 키워드 인덱스 추출\n",
    "    # 만약 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    \n",
    "    # 가장 높은 유사도를 가진 키워드 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 .. 중략 ..]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "    \n",
    "    # 최고 키워드는 이미 추출했으므로 top_n -1번만큼 아래를 반복\n",
    "    # ex ) top_n = 5라면, 아래 loop는 4번 반복됨\n",
    "    \n",
    "    for _ in range(top_n - 1) :\n",
    "        candidates_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis= 1)\n",
    "        \n",
    "        # MMR 계산\n",
    "        mmr = (1-diversity) * candidates_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "        \n",
    "        # keywords & candidates 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "        \n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "# mmr(doc_embedding, candidate_embeddings, candidates, top_n = 5, diversity = 0.7)\n",
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates) :\n",
    "    \n",
    "    # 문서와 각 키워드간 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    \n",
    "    # 각 키워드 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings,\n",
    "                                            candidate_embeddings)\n",
    "    \n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 선택\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "    \n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf \n",
    "    candidate = None\n",
    "    \n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n) :\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim :\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "            \n",
    "            \n",
    "    return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "def get_embeddings(doc) :\n",
    "    \n",
    "    okt = Okt()\n",
    "    # word tokenizing\n",
    "    tokenized_doc = okt.pos(doc)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "    \n",
    "    n_gram_range = (2, 3) # ngram 범위 설정\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "    \n",
    "    # model\n",
    "    model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "    doc_embedding = model.encode([doc])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    return doc_embedding, candidate_embeddings, candidates\n",
    "\n",
    "def get_doc_keyword(doc,mode,top_n, nr_candidates=15, diversity=0.7) :\n",
    "    doc_embedding, candidate_embeddings, candidates = get_embeddings(doc)\n",
    "    if mode == 'max_sum_sim' :\n",
    "        return max_sum_sim(doc_embedding, candidate_embeddings, candidates,top_n=top_n, nr_candidates=nr_candidates)\n",
    "    else :\n",
    "        return mmr(doc_embedding, candidate_embeddings, candidates, top_n = top_n, diversity=diversity)\n",
    "    \n",
    "# 뉴스 기사 가져와서 적용해보기 \n",
    "article = '''정부는 오미크론 변이 유행이 정점을 향해 가고 있다면서, 이번 유행의 정점이 코로나19에 대응하는 마지막 큰 위기가 될 수 있다고 16일 전망했다.\n",
    "이런 가운데 정부는 사회적 거리두기 조정을 위해 이날부터 본격적으로 일상회복지원위원회 의견 수렴에 나선다. 정부는 오는 21일부터 적용할 새로운 거리두기 조치를 18일 발표할 예정이다.\n",
    "손영래 보건복지부 중앙사고수습본부 사회전략반장은 16일 중앙재난안전대책본부 브리핑에서 \"정점이 예측대로 형성되면서 의료체계를 준비된 범위에서 대응할 수 있다면, 이번 위기가 코로나19 전반 대응 과정에서 가장 마지막의 큰 위기가 될 것으로 본다\"고 말했다.\n",
    "이날 0시 기준 신규확진자는 40만741명으로 처음으로 40만명을 넘어섰다. 위중증 환자도 1천244명으로 최다치를 기록했다. 전날 하루 사망자는 164명이다.\n",
    "손 반장은 \"오미크론 유행은 이제 정점을 향해 확진자 발생이 최대치로 증가하고 있는 상황\"이라며 \"금주 또는 늦어도 다음 주 정도가 이번 유행의 정점\"이라고 설명했다.\n",
    "정부는 복수의 연구기관 분석을 종합해 유행 정점이 16∼22일 형성되고, 정점에서 신규 확진자는 일평균 31만6천∼37만2천명 나올 수 있다고 전망했다.\n",
    "하루 확진자 수가 40만명을 넘었지만, 최근 1주일 평균으로는 34만5천242명이다. 손 반장은 유행 정점에서 신규확진자가 최대 37만2천명 발생한다는 예측도 '일평균'임을 유의해달라고 당부했다.'''    \n",
    "\n",
    "print('MAX_SUM_SIM Result:', get_doc_keyword(article,mode='max_sum_sim',top_n= 7))\n",
    "print('MMR Result:',get_doc_keyword(article,mode='mmr',top_n= 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a9091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
